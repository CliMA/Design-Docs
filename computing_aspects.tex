% !TEX root = main.tex

\section{Computing Aspects}
\label{sec:computing_aspects}

Let us decompose the computing aspects into the following groups
\begin{enumerate}
\item workflow language
\item parallelization API
\item many-core API
\item graph-partitioning and grid generation

\end{enumerate}

\subsection{Workflow Language}
Although Python is a popular workflow language, we will likely explore Julia as another option.  The reasons for possibly using Julia are: 
\begin{itemize}
\item We would like to use one language for both prototyping and workflow development,
\item Unlike Python, Julia was built for high-performance computing (see Sec.\ \ref{sec:computing_aspects/manycore}),
\item If we engage with the Julia community, we can get many good students to participate in various forms,
\item Risk-mitigation plan includes falling back to pure C code with Python scaffolding and OCCA kernels.
\end{itemize} 

\subsection{Parallelization API}
We propose to use the Message-Passing Interface (MPI) for communicating across processors and nodes (with multiple processors).  One of the NPS team members (Lucas or Jeremy?) has written Julia wrappers for MPI.

\subsection{Many-core API}
\label{sec:computing_aspects/manycore}
We propose to use a GPU library for accessing the GPUs. The NPS team has vast experience in this area. For example, the NPS team ported NUMA using OCCA2 with a CUDA back-end to run on the Titan supercomputer using 16,000 GPU cards achieving very good weak scaling (see \cite{abdi:2016b,abdi:2018}). The approach will be to write all of the compute-kernels in either GPUArrays (from Julia), OCCA2, or CUDA. From our perspective, the compute-kernel is the focus and most GPU-ready kernels look more or less the same (OCCA, CUDA, and OpenCL). 

\subsection{Graph-Partitioning and Grid Generation}
One of the NPS team members (Jeremy Kozdon) has written Julia wrappers to the p4est library developed by another NPS team member (Lucas Wilcox).  We propose to use p4est for both grid-generation and graph-partitioning or Metis with a specific grid generator for both the LES and global domains. The global domain will use a cubed-sphere grid while the LES domains will use a logically Cartesian cube grid.

\subsection{Code-base Repository}
The code is maintained in Github under the name: Climate Modeling Alliance.

\subsection{Software Management}
Software engineers are required in order to maintain the coupled software-base and to have a centralized person "in charge" of the software code.  It could also be helpful to have a support person who can dedicate time to help with HPC issues on running on the specific Caltech cluster that we will use.  

\subsection{Computing Hardware}
We propose to use about \$30K for purchasing a number of GPU cards.  Most of the development work will be performed on this new GPU cluster at NPS.  We will also  time available to us on the Caltech cluster. We also need to consider which types of computers we will be targeting (e.g., Summit at ORNL) although this will not happen until after year 1.
